{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain sentence-transformers faiss-cpu transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pdfplumber\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LOQ\\AppData\\Roaming\\Python\\Python312\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Load an embedding model\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')  # or choose another model\n",
    "\n",
    "# Load the document and split it into smaller sections\n",
    "document=extract_text_from_pdf('x.pdf')\n",
    "chunks = document.split(\". \")  # Simple split by sentence; customize as needed\n",
    "\n",
    "# Generate embeddings for each chunk\n",
    "embeddings = embedding_model.encode(chunks)\n",
    "\n",
    "# Build the FAISS index\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(np.array(embeddings))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<faiss.swigfaiss_avx2.IndexFlatL2; proxy of <Swig Object of type 'faiss::IndexFlatL2 *' at 0x00000164081D2490> >"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dear Hiring Manager,\\nI am writing to express my interest in the Applied Machine Learning Engineer position at\\nCerebras Systems. With a robust background in machine learning, deep learning, and\\nAI-powered solutions, I am excited about the opportunity to contribute to the groundbreaking\\nadvancements at Cerebras.\\nAt Valeo, I have developed and implemented AI-powered solutions that significantly reduced\\nmanual effort and enhanced accuracy in issue classification, leading to a 60% reduction in\\nJira SLA. My work included designing and deploying Jenkins pipelines for automated issue\\nclassification and fine-tuning large language models like Llama 3, Llama 2, Mistral, and\\nPaLM for optimizing software function allocation. These experiences have honed my skills in\\nleveraging advanced ML models and streamlining processes using tools like Docker, which I\\nbelieve align well with Cerebras’ mission to innovate at every level of the stack.\\nMy tenure at Omega Wireless further equipped me with the ability to develop advanced RF\\nplanning systems and optimize network configurations using algorithms such as Simulated\\nAnnealing. This role underscored the importance of efficiency and accuracy, qualities that I\\nam eager to bring to the development and optimization of state-of-the-art models on the\\nCerebras system.\\nI hold a Bachelor of Engineering degree in Electronics and Electrical Communication\\nEngineering from Cairo University, where I cultivated a solid foundation in machine learning,\\ndeep learning, and computer vision. My academic and professional experiences have made\\nme proficient in machine learning frameworks such as TensorFlow, Keras, and PyTorch, and\\nequipped me with the skills necessary to define custom layers and perform hyper-parameter\\ntuning.\\nCerebras’ mission to fundamentally change the way ML researchers work resonates with my\\ncareer aspirations. I am particularly drawn to the opportunity to work with one of the fastest\\nAI supercomputers in the world and to explore new model architectures that take advantage\\nof Cerebras’ unique capabilities. The prospect of contributing to a platform that is beyond the\\nconstraints of the GPU and participating in cutting-edge AI research is immensely exciting.\\nI am confident that my background and skills make me a strong fit for this role. I look forward\\nto the opportunity to discuss how my experience and vision align with the innovative work\\nbeing done at Cerebras.\\nThank you for considering my application. I am eager to bring my expertise to your team and\\ncontribute to the pioneering advancements at Cerebras Systems.\\nSincerely,\\nBassant Elsayed\\nMachine Learning Engineer\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sentence_transformers import SentenceTransformer\n",
    "# import faiss\n",
    "# import numpy as np\n",
    "\n",
    "# # Load an embedding model\n",
    "# embedding_model = SentenceTransformer('all-MiniLM-L6-v2')  # or choose another model\n",
    "\n",
    "# # Load the document and split it into smaller sections\n",
    "# document = \"Your document text goes here.\"\n",
    "# chunks = document.split(\". \")  # Simple split by sentence; customize as needed\n",
    "\n",
    "# # Generate embeddings for each chunk\n",
    "# embeddings = embedding_model.encode(chunks)\n",
    "\n",
    "# # Build the FAISS index\n",
    "# dimension = embeddings.shape[1]\n",
    "# index = faiss.IndexFlatL2(dimension)\n",
    "# index.add(np.array(embeddings))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sentence_transformers import SentenceTransformer\n",
    "# import faiss\n",
    "# import numpy as np\n",
    "\n",
    "# # Load an embedding model\n",
    "# embedding_model = SentenceTransformer('all-MiniLM-L6-v2')  # or choose another model\n",
    "\n",
    "# # Load the document and split it into smaller sections\n",
    "# document = \"Climate change refers to long-term shifts and alterations in temperature and weather patterns. Primarily caused by human activities, such as burning fossil fuels, deforestation, and industrial processes, climate change has far-reaching impacts. Greenhouse gases like carbon dioxide trap heat in the Earth's atmosphere, leading to global warming. Some of the effects of climate change include rising sea levels, more frequent extreme weather events, and shifts in ecosystems and wildlife populations. Efforts to mitigate climate change include transitioning to renewable energy, improving energy efficiency, and conserving natural resources.\"\n",
    "# chunks = document.split(\". \")  # Simple split by sentence; customize as needed\n",
    "\n",
    "# # Generate embeddings for each chunk\n",
    "# embeddings = embedding_model.encode(chunks)\n",
    "\n",
    "# # Build the FAISS index\n",
    "# dimension = embeddings.shape[1]\n",
    "# index = faiss.IndexFlatL2(dimension)\n",
    "# index.add(np.array(embeddings))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_chunk(question, index, chunks, model=embedding_model, k=3):\n",
    "    question_embedding = model.encode([question])\n",
    "    distances, indices = index.search(np.array(question_embedding), k)\n",
    "    return [chunks[idx] for idx in indices[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def retrieve_relevant_chunk(question, index, chunks, model=embedding_model, k=3):\n",
    "#     question_embedding = model.encode([question])\n",
    "#     distances, indices = index.search(np.array(question_embedding), k)\n",
    "#     return [chunks[idx] for idx in indices[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load a language model for answering questions\n",
    "qa_model = pipeline(\"text2text-generation\", model=\"t5-small\")  # Example: T5 model\n",
    "\n",
    "def answer_question(question, retrieved_chunks):\n",
    "    context = \" \".join(retrieved_chunks)\n",
    "    input_text = f\"question: {question} context: {context}\"\n",
    "    answer = qa_model(input_text)[0]['generated_text']\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LOQ\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\generation\\utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Sincerely, Bassant Elsayed Machine Learning Engineer My work included designing and deploying\n"
     ]
    }
   ],
   "source": [
    "question = \"who is Bassant\"\n",
    "retrieved_chunks = retrieve_relevant_chunk(question, index, chunks)\n",
    "answer = answer_question(question, retrieved_chunks)\n",
    "\n",
    "print(\"Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'detail': 'Not Found'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \" http://127.0.0.1:8000/upload/\"\n",
    "files = {'files': open('D:\\\\RAG\\\\rag_app\\\\x.pdf', 'rb')}\n",
    "response = requests.post(url, files=files)\n",
    "\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
